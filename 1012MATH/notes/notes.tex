\documentclass[11pt]{book}
\usepackage[margin=2cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\author{pearl}
\title{MATH1012 Notes}

\begin{document}
\maketitle
\chapter{Linear Algebra}
\section{Set of linear equations}
\subsection{Definitions}
\begin{itemize}
	\item{Linear equation: is an equation whose terms are numbers, and variables to the first power e.g $x+y=3$ is linear, but  $x^{2}+4y=5$ isn't.}
	\item{Solution: a solution is a choice of values (numbers) that satisfies a linear equation.}
	\item{System of linear Equation: is just a collection of linear equation. A solution to a system linear equation is a set of solutions which satisfy all equations. Give solution using set notation. E.g: $S=\{\left(2,1\right)\}$ for the graph: $x+2y=4, x-y=1$ an SLE in 2 dimensional space can either have:
	\begin{itemize}
		\item{No solutions (inconsistent): e.g 2 parallel lines}
		\item{unique solution: 2 intersecting lines}
		\item{Infinitely many solutions: 2 lines are the same line/overlay-ed on top of each other}
	\end{itemize}
	}
\end{itemize}
\subsubsection{Solving set of linear equations}
\par{Solve them geometrically or solve them through algebra. Example below:}
\begin{align*}
	x+y+z=1 \\ 
	x+y-z=1
\end{align*}
\subsubsection{Solving geometrically}
\begin{enumerate}
	\item{guess 1 point from each equation: (1,0,0) and (0,1,0)}
	\item{The line that connects up the points is a solution}
\end{enumerate}
\subsubsection{Solving algebraically}
\begin{enumerate}
	\item{Subtraction method. 'Subtract' one equation from the other
		\begin{align*}
			(x+y+z=1) - (x+y-z=1)\\
			\implies 0 + 0 + 2z=0 \\
			\implies z=0 \\
			\implies x+y+0=1 \\
			\implies x+y=1
		\end{align*}
		}
\end{enumerate}
\subsection{Parametric Solutions}
\par{In a parametric solution, every coordinate is either \textbf{constant, free variable, linear combination }(constants and free variables). i.e expressing the equation in terms of one variable. We can take x as the free variable, or y. Using the above as an example:}
\begin{align*}
	x+y+z=1 \\ 
	x+y-z=1 \\
	\implies x+y=1 \\
	\implies x=1-y \\
	\implies y=1-x \\
	\implies z=0 \\
	\therefore S=\{\left(x, 1-x, 0\right)\ \|x\in \mathbb{R} \}
\end{align*}
\par{Some variables cannot be chosen as free variables, in this example: z.}
\subsection{Elementary Row Operations}
\par{Given a set of linear equations: 
\begin{align*}
	x+y=5 \\
	2x-2y=7
\end{align*}
We can do the following and have no effect on the solution set:
\begin{itemize}
	\item{swap 2 equations' locations}
		\begin{align*}
			2x-2y=7 \\
			x+y=5
		\end{align*}
	\item{Multiply an equation by a non-zero number}
		\begin{align*}
			x+y=5 \\
			\text{becomes} \\
			2x+2y=2 \times 5 \\
			= 2x +2y=10
		\end{align*}
	\item{Add a multiple of one equation to another}
		\begin{align*}
			x+y=6 \\
			2x+2y=10 \\
		\end{align*}
		$R2\leftarrow R2-R1$
		\begin{align*}
			x+y=6 \\
			-4y=-3 \\
			y=\frac{-3}{-4} \\
			y=\frac{3}{4}
		\end{align*}
\end{itemize}
}
\subsection{Matrix notation and Gaussian elimination}
\par{Systems of linear equations can be expressed as a matrix, where the variables (x,y,z , etc) are omitted and only the coefficients are written down. Example:
\begin{align}
	x+2y=7 \\
	2x-y=4
\end{align}
Becomes:
\[
\left[
\begin{array}{cc|c}
	\text{x column} & \text{y column} & \text{solution} \\
	1 & 2 & 7 \\
	2 & -1 & 4
\end{array}
\right]
\]
\subsection{Row Echelon Form}
\par{A matrix is in row echelon form if:
	\begin{itemize}
		\item{The rows that are \textbf{all-zero} should be at the bottom}
		\item{The \textbf{leading entry} of every other row is \textbf{further to the right} than the leading entry of any previous rows}
		\item{The leading entry is the first number of each row}
		\item{\textbf{basic variables} are variables that are also leading entries.}
		\item{All non-basic variables are free variables}
	\end{itemize}
}
\subsubsection{Back substitution}
\par{Read the last row and it should give one of the answers. E.g We want to have a set of answers which are either:
\begin{itemize}
	\item{free variables (non basic)}
	\item{numbers}
	\item{a combination of both}
\end{itemize}}
\subsubsection{Gaussian Elimination}
\par{Example: }
\begin{itemize}
	\item{The pivot is the top row. We want to apply row operations to make the numbers below the pivot to 0}
\end{itemize}
\[
\left[
\begin{array}{cccc|c}
1 & 0 & 1 & 3 & 2 \\
2 & 1 & 2 & 0 & 0 \\
-1 & 3 & 4 & 0 & 1
\end{array}
\right]
\]
\begin{enumerate}
	\item{R2$\leftarrow$ R2-2R1}
	\item{R3$\leftarrow$ R3-2R1}
\end{enumerate}
\[
\left[
\begin{array}{cccc|c}
1 & 0 & 1 & 3 & 2 \\
0 & 1 & 0 & -6 & -4 \\
0 & 3 & 5 & 3 & 3
\end{array}
\right]
\]
\begin{enumerate}
	\item{R3 $\leftarrow$ R3 - 3 $\times$ R2}
\end{enumerate}
\[
\left[
\begin{array}{cccc|c}
1 & 0 & 1 & 3 & 2 \\
0 & 1 & 0 & -6 & -4 \\
0 & 0 & 5 & 21 & 15
\end{array}
\right]
\]
\par{Here we are now in reduced row echelon form. We can see that the $x_{4}$ column is non-basic. We want to express the basic variables in terms of the non basic variables.}
\subsubsection{Counting solutions}
\par{To determine whether the solution count is 0, 1 or $\infty$}
\begin{itemize}
	\item{If all numbers are 0 producing a non-zero, then there are no solutions}
	\item{if there is a free variable there are infinite solutions}
	\item{if there are no free variables and there doesn't exist a row containing all zeros producing a non-zero value in the augmented column then there is exactly 1 solution.}
\end{itemize}
\begin{itemize}
	\item{if SLE is \textbf{inconsistent then} $S =\emptyset$}
	\item{If SLE has a \textbf{unique solution}, then get S from the matrix}
	\item{if there are infinitely many solutions, then use back substitution to express S in \textbf{parametric form}}
	\item{S is the set of solutions}
\end{itemize}
\subsubsection{Reduced Row Echelon Form}
\par{There is a simpler form of matrices called reduced row echelon form. It has the extra condition from regular row echelon form in that both above and below the pivot row has to be 0. The leading entries must also be '1'. This form lets us get the direct parametric form in in the solution for back substitution.}
\begin{enumerate}
	\item{Ensure the entry in pivot position is non-zero (swap rows if necessary)}
	\item{Add a suitable multiple of pivot row to each row above and below the pivot row to zero out the matrix entries above and below the pivot}
	\item{Move the pivot position 1 column and 1 row}
\end{enumerate}
\section{Vector spaces and subspaces}
\begin{itemize}
	\item{The set of all vectors of \textbf{arity} n is denoted $\mathbb{R}^{n}$}
	\item{adding vectors of the same arity}
	\item{multiplying vectors by a number}
	\item{Set of solutions of SLE is just a vector}
\end{itemize}
\subsection{Closure}
\par{A set of vectors that are closed under addition if any vector from that set added to another vector in that set will produce a vector that exists within its original set. This also applies to multiplication: a vector multiplied by a number will stay in its original set. Closed in general is a property of a set of vectors for which when an operation is applied to such set, the solution is part of the set. This property must hold for all members of the set. E.g:
\begin{align*}
	S=\{\left(1,2\right), \left(2,4\right), \left(3,6\right)\} \\
	\text{The above is not closed as not all pairs produce addition answers within the set}
\end{align*}
in order to show closure, provide a specific counterexample that always works, or symbolic proof. THe proof must work for all examples, whereas the disproof only needs to show 1 counterexample.}
\subsection{Subspace}
\par{A set of vectors is a subspace if
\begin{itemize}
	\item{The zero vector must be contained in the set.}
	\item{The set must be closed under addition/multiplication.}
\end{itemize}
Testing if a subspace does not contain a zero, is a fast way to test if a subspace is valid. In $\mathbb{R}^{2}$, valid subspaces are typically the full set, the origin, or a line through the origin. E.g:
\begin{itemize}
	\item{$x=y$ in $\mathbb{R}^{2}$}
	\item{$x+1=2y$ in $\mathbb{R}^{2}$}
\end{itemize}
\subsubsection{Proving $x=y$}
\begin{align*}
	x+2y-3z=0 \\
	(0)+2(0)-3(0)=0 \\
	\implies (0,0,0) \text{is within the set} \\
	\text{let } u,v \in S\\
	\text{then} \left(u_{1}+u_{1},u_{2}+u_{2},u_{3}+u_{3}\right) \\
	u_{1}+v_{1}+2\left(u_{1}+v_{1}\right)-3\left(u_{1}+v_{1}\right)
\end{align*}
}

\subsubsection{Proving the validity of subspaces}
\par{The best way to prove the validity of a subspace is to check the 3 criteria as listed above.}
\begin{itemize}
	\item{The origin, or zero vector $\left(0,0,0....0_{n}\right)$ is included for $S\in\mathrm{R}^{n}$. This is often the easiest check}
	\item{$S$ must be closed under addition. This means if you take 2 vectors within the subset, the result must still be within the subspace.}
	\item{$S$ must be closed under scalar multiplication. This means if you take a vector in the set and multiply it by any scalar, it must stay within the set.}
\end{itemize}
\subsubsection{How to check each condition}
\par{Example: Given $S\in\mathrm{R^{3}}, x+2y-3z=0$}
\begin{enumerate}
	\item{The first step is to decide whether the origin/$\vec{0}$ is in the subset. The easiest way to check is to substitute the $\vec{0}$ of the current arity into the linear equation and see if it holds up. \textbf{Here the 0 vector is in the set because:}}
\begin{align*}
	& \left(0,0,0\right)\rightarrow x+2y-3z=0 \\
	& \left(0\right)+2\left(0\right)-3\left(0\right)=0 
\end{align*}
\item{Next we must show that if we take any 2 solutions, and we add them together, we get another solution within the set. Start by defining 2 elements within the set, let $\vec{u}=\left(u_{1},u_{2},u_{3}\right)\in S$ and let $\vec{v}=\left(v_{1},v_{2},v_{3}\right)\in S$ We do not know anything about u or v, except they are both vectors that exist in $S$. They must exist in  $S$ as that is they are hypothetical solutions of S.}
	\begin{enumerate}
		\item{ let: $\vec{u}=\left(u_{1},u_{2},u_{3}\right)\in S$ and let $\vec{v}=\left(v_{1},v_{2},v_{3}\right)\in S$ }
		\item{ then: $u_{1}+u_{2}-3u_{3}=0$ and $v_{1}+v_{2}-3v_{3}=0$  due to $\vec{u}$ and $\vec{v}\in S$ }
		\item{ The above line shows that the 2 vectors we are using are in the set of solutions of $S$ }
		\item{ Check if: $\vec{u}+\vec{v}=\left(u_{1}+v_{1},u_{2}+v_{2},u_{3}+v_{3}\right)$ Gives the solution of the equation we are testing}
		\item{ Demonstrate the following is equal to zero, but do not state it beforehand.}
		\item{ $\vec{u}+\vec{v}=u_{1}+v_{1}+2\left(u_{2}+v_{2}\right)-3\left(u_{3}+v_{3}\right)$}
		\item{ After rearranging, we get: $\vec{u}+\vec{v}=u_{1}+u_{2}-3u_{3}+v_{1}+v_{2}-3v_{3}$}
		\item{ Which we determined in step 2 to be $\vec{u}+\vec{v}=0+0=0$}
		\item{ We have deduced that both u and v are in the set, and that u+v are also in the set, therefore it is closed under addition.}
	\end{enumerate}
	\item{Next we must show that if we take any scalar value and multiply it by any vector, in this case we can use $u$ from the previous step, as we have already proved it is in the set  $S$, that it remains within the set of solutions. }
	\begin{enumerate}
		\item{let $\alpha$ be any scalar multiple}
		\item{$\alpha\vec{u}$}
		\item{$\vec{u}=0$ as shown above, as it is a solution}
		\item{$\alpha 0=0$}
	\end{enumerate}
	\item{The solution of the previous did form a subspace}
\end{enumerate}
\par{Consider the next linear equation $x+2y-3z=10$. This fails the first test of not containing the origin/0 vector. Even if we try to test for closure under addition, we get that that test fails too. This is because the right hand side of the equation is non-zero, i.e: there is a constant within the linear equation, i.e, the solution is non homogeneous. The solutions to a homogeneous system of linear equations \textbf{always forms a subspace}. The inverse of that is also true, in that \textbf{the solutions to a non-homogeneous set of linear equations is never a subspace}.}
\subsection{What are all the subspaces of $\mathbb{R}^{2}$?}
\begin{itemize}
	\item{$S=\lbrace\left(0,0\right)\rbrace$ is a subspace, as it contains the 0 vector and is closed both under addition and multiplication.}
	\item{Any straight line going through the origin is a subspace, as it contains the origin by design. If you added any 2 points on the line, as the gradient remains the same, it will just end up somewhere else on the line. This is the same for a scalar multiple}
	\item{$\mathbb{R}^{2}$ Itself, given enough points added/large enough scalar, we can 'tile' the whole of $\mathbb{R^{2}}$, therefore itself must be a subspace.}
\end{itemize}
\subsubsection{What are all the subspaces of $\mathbb{R}^{3}$?}
\par{A similar argument is made for $\mathbb{R}^{3}$ as for $\mathbb{R}^{2}$}
\begin{itemize}
	\item{The origin itself: $S=\lbrace(\left(0,0,0\right)\rbrace)$}
	\item{Planes through the origin}
	\item{Lines through the origin}
	\item{The whole of $\mathbb{R}^{3}$}
\end{itemize}
\subsection{Linear combinations}
\par{A linear combination of $v$ and $w$ is any vector in the form: 
	\[
	\alpha v+\beta w
	\] 
Where $\alpha,\beta\in\mathbb{R}$}
\section{Span and Spanning set}
\par{Given: $T=\lbrace(v_{1},v_{2},v_{3}...v_{k}\rbrace)$, the \textbf{span} of $T$ is:
\[
	\text{span($T$)}=\lbrace\left(\alpha_{1}v_{1}+\alpha_{2}v_{2}+\alpha_{3}v_{3}...\alpha_{k}v_{k}\right): a_{i}\in\mathbb{R}\rbrace
\] 
The span of $T$ is exactly the set of all linear combinations that can be formed from the vectors in $T$, which forms a subspace (The span of anything is a subspace, and the Span of T is the smallest subspace containing T). 

The span of an empty set is defined to be 0: span$\left(\emptyset\right)=0$.
}
\subsection{Testing For Membership Of Span And The Spanning Set}
\par{To test for membership of 1 vector in a span, we simply use a set of linear equations. E.g:}
Is: $\left(1,0,1\right)\in\text{span}\left(\lbrace\left(1,2,3\right),\left(2,3,4\right)\rbrace\right)$ ?
$$\alpha\left(1,2,3\right)=\beta\left(2,3,4\right)=\left(1,0,1\right)$$
$$
\begin{pmatrix}
	\alpha +2\beta=1 \\
	2\alpha+3\beta=0 \\
	3\alpha+4\beta=1 \\
\end{pmatrix}
$$
Remember the span returns the full set, whereas the spanning set refers to the set which builds the span.
\subsection{Verifying a Spanning Set}
\par{Given a subspace $S$, how do we prove that $T$ is a spanning set? We must verify that all vectors in the span can be made from linear combinations of the spanning sets, as well as the spanning sets being unable to make anything outside of the span. E.g:

	Show that span$\left(\left(1,1,0\right), \left(0,0,1\right)\right)=\lbrace(\left(x,x,z\right):x,z\in\mathbb{R}\rbrace)$
}
\begin{enumerate}
	\item{What vectors are in: span$\left( \left(1,1,0\right), \left(0,0,1\right)\right)=\lbrace( \left(x,x,z\right) :x,z\in\mathbb{R} \rbrace)$}
	\item{All vectors of the form: $\alpha\left(1,1,0\right)+\beta\left(0,0,1\right)=\left(\alpha,\alpha,\beta\right)$}
	\item{This is similar to the above answer of $\left(x,x,z\right)$, so it is the same.}
\end{enumerate}
\subsection{Finding a small spanning set for $S$}
\begin{enumerate}
	\item{Algorithm: choose some vector $v_{1}\in S$ and examine span$\left(v_{1}\right)$}
	\item{Check if this is enough to fill $S$. If it isn't enough, there are some vectors that you can span, and some which you can't.}	
	\item{Choose a $v_{2}\in S$ to try and reach all the vectors that $v_{1}$ cannot reach.}
	\item{Keep going until S can be contained}
\end{enumerate}
\par{If $S$ is given in a \textbf{parametric form} then there is an easy way to find the spanning set.}
\begin{align*}
	& S=\lbrace\left(x,2x,z,-z\right): x,z,\in \mathbb{R}\rbrace \\
	& \vec{v_{1}}=\left(1,2,0,0\right) \\
	& \vec{v_{2}}=\left(0,0,1,-1\right) \\
	& \left(x,2x,z,-z\right)=\left(x,2x,0,0\right)+\left(0,0,z,-z\right)
\end{align*}
\section{Linear Independence}
\par{A set of vectors is linearly dependent if one of them is a linear combination of the others. E.g: \[
\lbrace\vec{v_{1}}, 2\vec{v_{1}}\rbrace
\] We see here that the 2nd coordinate is just 2 times the first coordinate. A set of size 2 is linearly dependent only when one is a multiple of the other.}
\subsection{Linear Independence Test}
\par{in $\mathbb{R}^{n}$, a set $T=\lbrace v_{1}, v_{2}, v_{3}...v_{k}\rbrace$ is linearly independent if and only if the homogeneous system of n linear equations in the unknowns has a unique solution. E.g: }
\begin{align*}
	& T=\lbrace\left(1,0,0\right), \left(0,1,1\right), \left(1,1,2\right)\rbrace \\
	& \text{Solve } \\ 
	& \alpha_{1}\left(1,0,0\right)+\alpha_{2}\left(0,1,1\right)+\alpha_{3}\left(1,1,2\right)=\left(0,0,0\right)
\end{align*}
The solution must always equal $\left(0,0,0\right)$ as it should be homogeneous
\subsection{Properties of linear independence}
\begin{itemize}
	\item{A subset of a linearly independent set is linearly independent.}
	\item{A superset of a linearly dependent set is dependent.}
\end{itemize}
\par{This is quite common sense as if you have a dependent set, adding something extra won't change it to be independent. Another idea is that the span would not change if a dependent vector is removed from the set of vectors. 

Suppose we have a set 'S' with multiple vectors within, but $v_{1}$ is linearly dependent on a linear combination of the other vectors: \[
S=\lbrace v_{1}, v_{2}, v_{3}... v_{k}\rbrace 
\] \par{Then, as $v_{1}$ is a linear combination of some other vectors: \[
 v_{1}=\alpha_{1}v_{1}+...\alpha_{k}v_{k}
\] We can see that by removing $v_{1}$ the span of S will not change, as whatever $v_{1}$ can make can already be made by the vectors which make up $v_{1}$: \[
\mathrm{span}\left(S\right)=\mathrm{span}\left(S\backslash\vec{v_{1}}\right)
\] }
\begin{itemize}
	\item{Linearly dependent set}
		\begin{itemize}
			\item{Set has \textbf{redundancy}}
			\item{Can remove dependent vectors without changing the span (as explained above}
		\end{itemize}
	\item{Linearly independent set}
		\begin{itemize}
			\item{No redundancy}
			\item{every vector is needed}
		\end{itemize}
\end{itemize}
\section{Basis}
\par{Let $S$ be a subspace of $\mathbb{R}^{n}$, then a \textbf{basis} for $S$ is a set $B$ of vectors in $S$ such that 
\begin{itemize}
	\item{$S=\mathrm{span}\left(B\right)$}
	\item{$B$ is linearly independent}
\end{itemize}
B shall contain exactly the vectors you need to \textbf{span} $S$, or to create every element within $S$. A basis is the \textbf{most efficient way} to specify a subspace. Given a basis, you can determine which vectors are in the span of that basis or not. }
\subsubsection{The Standard Basis}
\par{The standard basis is the most simple basis for $\mathbb{R}^{n}$, given by:}
\begin{align*}
	& e_{1}=\left(1,0,0,....,0\right) \\
	& e_{2}=\left(0,1,0,....,0\right) \\
	& e_{3}=\left(0,0,1,....,0\right) \\
	& e_{n}=\left(0,0,0,....,1\right) \\
\end{align*}
\subsection{Dimension}
\par{Every basis for a subspace $S$ has the  \textbf{same number} of vectors. This number is called the dimension of $S$, denoted simply as  $\mathrm{dim}\left(S\right)$. E.g: anything spanned by 1 vector is an one-dimensional object, everything spanned by 2 vectors is a 2-dimensional object.

A basis for a subspace satisfies the "Goldilocks property".
\begin{itemize}
	\item{It is only \textbf{just big enough} to be a spanning set}
	\item{it is only \textbf{just small enough} to be independent}
\end{itemize}
Let $S$ be a $k$-dimensional subspace of $\mathbb{R}^{n}$. Then:
\begin{itemize}
	\item{Any set of \textbf{less than} k is not a spanning set of $S$}
	\item{Any spanning set for $S$  \textbf{contains} a basis for $S$}
	\item{Any spanning set for $S$ of size \textbf{exactly} $k$ is a basis for $S$}
\end{itemize}
}
\subsubsection{Example}
\par{Give a basis for the solution space of the SLE: }
\begin{align*}
	& a+b+2c+d-e=0 \\
	& 2a+2b+5c+d-2e=0 \\
\end{align*}
\[
	\left[
	\begin{array}{ccccc|c}
		1 & 1 & 2 & 1 & -1 & 0 \\
		2 & 2 & 5 & 1 & -2 & 0
	\end{array}
	\right]
\] 
\par{A single row operation gives the following: $R2\leftarrow R2-2R1$}
\[
	\left[
		\begin{array}{ccccc|c}
			1 & 1 & 2 & 1 & -1 & 0 \\
			0 & 0 & 1 & -1 & 0 & 0
		\end{array}
	\right]
\] 
\par{From the above, we see a and c (first and 3rd column) are basic variables, whereas b, d and e are non-basic/free variables. We now do back substitution where :
\begin{itemize}
	\item{e is a parameter}
	\item{d is a parameter}
	\item{c is a basic variable, which from the 2nd row can be expressed as: $c-d=0$, therefore  $c=d$}
	\item{a is basic, so from the first row, we get $a+b+2c+d-e=0$, as $c=d$, we get  $a=e-b-3d$}
\end{itemize}
Every variable is now expressed as a free variable. There are 3 free variables, so the solution space is 3 dimensional. $S=\lbrace\left(e-b-3d, b, d, d, e\right)|b,d,e\in\mathbb{R}\rbrace$}. The parametric form gives us the basis in the following: 
\begin{align*}
& S=\lbrace\left(e-b-3d, b, d, d, e\right)|b,d,e\in\mathbb{R}\rbrace \\
& =b\left(-1,1,0,0,0\right)+d\left(-3,0,1,1,0\right)+e\left(1,0,0,0,1\right)
\end{align*}
This is because for b, the first term contains a negative b, and the 2nd term a positive b, for d, the first term contains a negative 3, and the 3rd and 4th terms, a positive d, and finally for e: the first and last terms contain an e. Because of back substitution, they are forced to be linearly independent, they span the whole set, so they are the basis for the solution space: \[
B =\lbrace\left(-1,1,0,0,0\right),\left(-3,0,1,1,0\right),\left(1,0,0,0,1\right)\rbrace
\]
\chapter{Matrix Algebra}
\par{Matricies can be added, multiplied by other matrices or by scalars or transposed. }
\subsection{Transposition}
\par{To transpose a matrix (denoted by the operation $\left(AB\right)^{T}$, we take each row and write it as its corresponding column}
\subsection{Addition}
\par{Matricies can be added simply like vector addition}
\subsection{Matrix Multiplication}
\section{Basic square Matricies}
\par{There are 2 square matrices that are defined}
\subsection{The 'zero' matrix}
\par{The zero matrix is just any matrix made up of only '0'. It is denoted by $O_{n}$, where no is the number of rows and columns}
\subsection{The 'identity' matrix}
\par{The identity matrix is defined as a square matrix of a 1 in the diagonal line: (identity matrix of size 3, or $I_{n}$)}
\[
	\left[
		\begin{matrix}
			1 & 0 & 0 \\
			0 & 1 & 0 \\
			0 & 0 & 1 
		\end{matrix}
	\right]
\] 
\par{Reminder that the identity matrix acts as a '1' in the matrix theory, whereas the zero-matrix acts as a zero. This means that identity matrices' multiplication are commutative: }
\[
A\cdot I=I\cdot A=A
\] Where A is any matrix and I is the identity matrix of the same order \[
A+O=O+A=A
\] 
\subsection{Special Square Matricies}
\begin{itemize}
	\item{A matrix is \textbf{symmetric} if $A=A^{T}$}
	\item{A matrix is \textbf{skew-symmetric} if $A=-A^{T}$}
	\item{A matrix is \textbf{upper-triangular} if $A_{ij}=0$ for $i>j$ \[
	\left[
	\begin{matrix}
		x & y & z \\
		0 & a & b \\
		0 & 0 & c
	\end{matrix} 
	\right]
	\] }
	\item{A matrix is \textbf{lower-triangular} if $A_{ij}=0$ for $i<j$}
	\item{A matrix is \textbf{diagonal} if $A_{ij}$ for all $i\ne j$}
\end{itemize}
\subsection{SLE and matrices}
\par{We have $Ax=b$, where a is the coefficients, x is the variables and b is the values to the right of the augmentation bar. Let A be an $m\times n$, m rows n columns matrix. 
\begin{itemize}
	\item{The \textbf{Row Space of A} is the span of the rows, the set of all linear combinations of all the rows. It is a subspace of $\mathrm{R}^{n}$.}
	\item{The \textbf{Column Space of A} is the set of all linear combinations of its columns, the span of the columns. They are solved similarly to the row space, simply transpose prior to solving and then transpose back to obtain the answer. It is a subspace of $\mathrm{R}^{m}$}
	\item{The \textbf{Null Space of A} is the set of all solutions to $Ax=0$, it is a subspace of $\mathrm{R}^{n}$. To find the null space, you just have to solve the system of linear equations.}
\end{itemize}
}
\subsubsection{Dimensions}
\par{The row rank of A is the dimension of its row space. The column rank of A is the dimension of its column space and the nullity of A is the dimension of its null space. \[
\begin{matrix}
	1 & 2 & -1 & 3 \\
	1 & 0 & 3 & -1
\end{matrix}
\] The \textbf{row rank} is 2. The \textbf{column rank} is the whole of 2. THe row rank is always equal to the column rank. The \textbf{Nullity} is given by: \[
\left[
	\begin{array}{cccc|c}
	1 & 2 & -1 & 3 & 0\\
	1 & 0 & 3 & -1 & 0
\end{array}
\right]
\] \[
\left[
\begin{array}{cccc|c}
	1 & 2 & -1 & 3 & 0 \\
	0 & -2 & 4 & -4 & 0
\end{array}
\right]
\] }
\subsubsection{Example}
\par{We have a $3\times4$ matrix: } 
%\[
%\left[
%	\begin{array}
%		1 & 0 & 1 & 1 \\
%		1 & 1 & 3 & 2 \\
%		2 & 1 & 4 & 3 
%	\end{array}
%\right]
%\]
The third row is dependent on the first 2, so everything built with the 3rd row can be replaced by a combination of 1 and 2. Therefore the basis is just the first and the 2nd row: \[
B=\lbrace\left(1,0,1,1\right),\left(1,1,3,2\right)\rbrace
\] 



\end{document}

